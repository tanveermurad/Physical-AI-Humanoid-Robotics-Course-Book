"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[8333],{8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const r={},o=t.createContext(r);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},8783:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"part2-ai-for-robotics/learning","title":"Learning","description":"\\"Learning\\" in robotics refers to the ability of a robot to improve its performance on a task over time, often without explicit programming for every possible scenario. This is primarily achieved through various machine learning techniques, allowing robots to adapt, make decisions, and interact more intelligently with complex and unpredictable environments.","source":"@site/docs/part2-ai-for-robotics/learning.md","sourceDirName":"part2-ai-for-robotics","slug":"/part2-ai-for-robotics/learning","permalink":"/Physical-AI-Humanoid-Robotics-Course-Book/docs/part2-ai-for-robotics/learning","draft":false,"unlisted":false,"editUrl":"https://github.com/tanveermurad/Physical-AI-Humanoid-Robotics-Course-Book/tree/main/my-book/docs/part2-ai-for-robotics/learning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Perception","permalink":"/Physical-AI-Humanoid-Robotics-Course-Book/docs/part2-ai-for-robotics/perception"},"next":{"title":"Bipedal Locomotion","permalink":"/Physical-AI-Humanoid-Robotics-Course-Book/docs/part3-humanoid-robotics/bipedal-locomotion"}}');var r=i(4848),o=i(8453);const s={sidebar_position:3},a="Learning",l={},c=[{value:"Reinforcement Learning (RL)",id:"reinforcement-learning-rl",level:2},{value:"Key Concepts:",id:"key-concepts",level:3},{value:"Algorithms:",id:"algorithms",level:3},{value:"Application in Robotics:",id:"application-in-robotics",level:3},{value:"Supervised Learning",id:"supervised-learning",level:2},{value:"Key Concepts:",id:"key-concepts-1",level:3},{value:"Algorithms:",id:"algorithms-1",level:3},{value:"Application in Robotics:",id:"application-in-robotics-1",level:3},{value:"Unsupervised Learning",id:"unsupervised-learning",level:2},{value:"Key Concepts:",id:"key-concepts-2",level:3},{value:"Application in Robotics:",id:"application-in-robotics-2",level:3},{value:"Challenges and Future of Learning in Robotics",id:"challenges-and-future-of-learning-in-robotics",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"learning",children:"Learning"})}),"\n",(0,r.jsx)(n.p,{children:'"Learning" in robotics refers to the ability of a robot to improve its performance on a task over time, often without explicit programming for every possible scenario. This is primarily achieved through various machine learning techniques, allowing robots to adapt, make decisions, and interact more intelligently with complex and unpredictable environments.'}),"\n",(0,r.jsx)(n.h2,{id:"reinforcement-learning-rl",children:"Reinforcement Learning (RL)"}),"\n",(0,r.jsx)(n.p,{children:'Reinforcement Learning is a powerful paradigm where an "agent" (the robot) learns to make decisions by performing "actions" in an "environment" to maximize a cumulative "reward." It\'s like training a pet: good behaviors are rewarded, leading the pet to repeat them.'}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts",children:"Key Concepts:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Agent"}),": The robot itself, which perceives the environment and takes actions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment"}),": The world the robot operates in."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State"}),": A snapshot of the environment at a given time."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"}),": A movement or decision made by the agent."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reward"}),": A numerical signal received by the agent after taking an action, indicating how good or bad that action was. The goal is to maximize total reward."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"algorithms",children:"Algorithms:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Q-learning"}),': An off-policy RL algorithm that learns the value of taking a certain action in a given state. The "Q-value" represents the expected future reward.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Policy Gradients"}),': Algorithms that directly learn a "policy" (a mapping from states to actions) that maximizes the expected reward.']}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"application-in-robotics",children:"Application in Robotics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Locomotion"}),": Learning complex walking gaits for bipedal and quadrupedal robots."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Manipulation"}),": Learning how to grasp, lift, and place objects, especially in unstructured environments."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task Learning"}),": Robots learning to complete sequences of actions to achieve higher-level goals, such as assembling components or clearing a table."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,r.jsx)(n.p,{children:'Supervised learning involves training a model on a dataset of "labeled examples," meaning each input has a corresponding correct output. The model learns to map inputs to outputs, and then generalizes this mapping to new, unseen data.'}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts-1",children:"Key Concepts:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Training Data"}),": A collection of input-output pairs used to teach the model."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Features"}),": The input variables or characteristics of the data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Labels"}),": The correct output values for each input."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"algorithms-1",children:"Algorithms:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Neural Networks (Deep Learning)"}),": Particularly Convolutional Neural Networks (CNNs) for image data, widely used for their ability to learn complex patterns."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Support Vector Machines (SVMs)"}),": Algorithms that find the optimal hyperplane to separate different classes of data."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"application-in-robotics-1",children:"Application in Robotics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Recognition"}),": Identifying and classifying objects in a robot's environment using camera data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Motion Prediction"}),": Predicting the future movements of humans or other robots based on observed patterns."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic Segmentation"}),": Understanding which pixels in an image belong to which objects."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"unsupervised-learning",children:"Unsupervised Learning"}),"\n",(0,r.jsx)(n.p,{children:"Unsupervised learning deals with unlabeled data, where the model tries to find hidden patterns or structures within the data on its own."}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts-2",children:"Key Concepts:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Clustering"}),": Grouping similar data points together."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dimensionality Reduction"}),": Reducing the number of input variables while preserving important information."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"application-in-robotics-2",children:"Application in Robotics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Anomaly Detection"}),": Identifying unusual sensor readings or robot behaviors that might indicate a fault."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature Extraction"}),": Discovering useful features from raw sensor data (e.g., visual features from images) that can be used by other learning algorithms."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"challenges-and-future-of-learning-in-robotics",children:"Challenges and Future of Learning in Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Despite significant progress, learning in robotics faces challenges:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Efficiency"}),": RL often requires vast amounts of data, which is expensive and time-consuming to collect in the real world."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sim-to-Real Gap"}),": Models trained in simulation often struggle when deployed on physical robots due to differences in physics and sensor noise."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety and Robustness"}),": Ensuring learned behaviors are safe and reliable, especially in human-robot interaction."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The future of learning in robotics lies in combining these approaches, leveraging human demonstrations (imitation learning), and developing more data-efficient and robust algorithms that can generalize across tasks and environments."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);